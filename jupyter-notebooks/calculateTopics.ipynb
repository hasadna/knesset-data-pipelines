{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example flow for processing and aggregating stats about committee meeting attendees and protocol parts\n",
    "\n",
    "See the [DataFlows documentation](https://github.com/datahq/dataflows) for more details regarding the Flow object and processing functions.\n",
    "\n",
    "Feel free to modify and commit changes which demonstrate additional functionality or relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit processing of protocol parts for development, -1 means no limit.\n",
    "PROCESS_PARTS_LIMIT = 100\n",
    "\n",
    "# Knesset num to query\n",
    "KNESSET_NUM = 20\n",
    "\n",
    "# Enable caching of protocol parts data (not efficient, should only be used for local development with sensible PROCESS_PARTS_LIMIT)\n",
    "PROCESS_PARTS_CACHE = True\n",
    "\n",
    "# Filter the meetings to be processed, these kwargs are passed along to DataFlows filter_rows processor for meetings resource\n",
    "MEETINGS_FILTER_ROWS_KWARGS = {'equals': [{'KnessetNum': KNESSET_NUM}]}\n",
    "\n",
    "# Don'e use local data - loads everything from knesset data remote storage\n",
    "# When set to False - also enables caching, so you won't download from remote storage on 2nd run.\n",
    "USE_DATA = False\n",
    "USE_CACHE = not USE_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_topic_to_set(topic_name):\n",
    "    lines = open(os.path.join(dir_name, topic_name + \".txt\"), 'r').readlines()\n",
    "    return set([line.strip().replace(\"\\ufeff\", \"\") for line in lines])\n",
    "\n",
    "dir_name = \"../topics/lexicons\"        \n",
    "\n",
    "files = os.listdir(dir_name)\n",
    "\n",
    "topics = [file.split('.')[0] for file in files]\n",
    "\n",
    "lexicons = {}\n",
    "for topic_name in topics:\n",
    "    lexicons[topic_name] = read_topic_to_set(topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/members/mk_individual/datapackage.json\n",
      "using cache data from .cache/members-mk-individual-names\n",
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/people/committees/meeting-attendees/datapackage.json\n"
     ]
    }
   ],
   "source": [
    "from dataflows import filter_rows, cache\n",
    "from datapackage_pipelines_knesset.common_flow import load_knesset_data, load_member_names\n",
    "\n",
    "# Loads a dict containing mapping between knesset member id and the member name\n",
    "member_names = load_member_names(use_data=USE_DATA)\n",
    "\n",
    "# define flow steps for loading the source committee meetings data\n",
    "# the actual loading is done later in the Flow\n",
    "load_steps = (\n",
    "    load_knesset_data('people/committees/meeting-attendees/datapackage.json', USE_DATA),\n",
    "    filter_rows(**MEETINGS_FILTER_ROWS_KWARGS)\n",
    ")\n",
    "\n",
    "if not USE_DATA:\n",
    "    # when loading from URL - enable caching which will skip loading on 2nd run\n",
    "    path = '.cache/people-committee-meeting-attendees-knesset-{}'.format(KNESSET_NUM)\n",
    "    load_steps = (cache(*load_steps, cache_path=path),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the datapackages which will be loaded\n",
    "\n",
    "Last command's output log should contain urls to datapackage.json files, open them and check the table schema to see the resource metadata and available fields which you can use in the processing functions.\n",
    "\n",
    "Check the [frictionlessdata docs](https://frictionlessdata.io/docs/) for more details about the datapackage file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract topics from lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataflows import Flow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "first = True\n",
    "running_index = 0\n",
    "\n",
    "meeting_data_global = None\n",
    "topics_df = None\n",
    "\n",
    "stats = defaultdict(int)\n",
    "member_attended_meetings = defaultdict(int)\n",
    "rows = []\n",
    "m_rows = []\n",
    "\n",
    "def initialize_meeting_data_global(meeting_row):\n",
    "    global meeting_data_global\n",
    "    global running_index\n",
    "    topics_exists = meeting_row['topics'] is not None\n",
    "    topics = \";\".join(meeting_row['topics']) if topics_exists else \"\"\n",
    "\n",
    "    meeting_data_global = {\n",
    "        'KnessetNum': [meeting_row['KnessetNum']],\n",
    "        'Year': [str(meeting_row['StartDate']).split(\"-\")[0]],\n",
    "        'Month': = df['StartDate'].apply(lambda l: str(l).split(\"-\")[1])\n",
    "        'CommitteeSessionID': [meeting_row['CommitteeSessionID']],\n",
    "        'Number': [meeting_row['Number']],\n",
    "        'Mks': [';'.join(meeting_row['mks'])] if meeting_row['mks'] is not None else [\"\"],\n",
    "        'Topics': [topics],\n",
    "        'StartDate': [meeting_row['StartDate']],\n",
    "        'CommitteeID': [meeting_row['CommitteeID']]\n",
    "    } \n",
    "    \n",
    "    # Adding topic counts in the 'topics' column \n",
    "    if(topics_exists):\n",
    "        topic_words = topics.split()\n",
    "        topic_words_size_2 = [\" \".join(topic_words[i:i+2]) for i in range(len(topic_words) - 2)]\n",
    "        topic_words_size_3 = [\" \".join(topic_words[i:i+3]) for i in range(len(topic_words) - 3)]\n",
    "      \n",
    "    for topic_name, lexicon in lexicons.items():\n",
    "        if not topics_exists:\n",
    "            meeting_data_global[topic_name + \"_score\"] = [0]\n",
    "        else:\n",
    "            count = lexicon_count(lexicon, topic_words) + lexicon_count(lexicon, topic_words_size_2) + lexicon_count(lexicon, topic_words_size_3)\n",
    "            meeting_data_global[topic_name + \"_score\"] = [count*3]\n",
    "    \n",
    "def word_permutations(word):\n",
    "    clean_word = word.strip()\n",
    "    permutations = [clean_word]\n",
    "    if len(word) > 1 and word.startswith('ה') or word.startswith('ב') or word.startswith('ל'):\n",
    "        permutations.append(word[1:])\n",
    "    return permutations\n",
    "\n",
    "\n",
    "def in_lexicon(word, lexicon):\n",
    "    for p in word_permutations(word):\n",
    "        if p in lexicon:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "            \n",
    "def lexicon_count(lexicon, words):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if in_lexicon(word, lexicon):\n",
    "            count += 1\n",
    "    return count   \n",
    "\n",
    "\n",
    "def process_meeting_protocol_part(row):\n",
    "    global meeting_data_global\n",
    "    global running_index\n",
    "    global rows\n",
    "    rows.append(row)\n",
    "    stats['processed parts'] += 1\n",
    "    if 'header' in row and row['header'] is not None:\n",
    "        words = row['header'].split()\n",
    "    else:\n",
    "        words = []\n",
    "    if 'body' in row and row['body'] is not None:\n",
    "        words += row['body'].split()      \n",
    "    words_size_2 = [\" \".join(words[i:i+2]) for i in range(len(words) - 2)]\n",
    "    words_size_3 = [\" \".join(words[i:i+3]) for i in range(len(words) - 3)]\n",
    "                    \n",
    "    for topic_name, lexicon in lexicons.items():\n",
    "        meeting_data_global[topic_name + \"_score\"][0] += lexicon_count(lexicon, words)\n",
    "        meeting_data_global[topic_name + \"_score\"][0] += lexicon_count(lexicon, words_size_2)  \n",
    "        meeting_data_global[topic_name + \"_score\"][0] += lexicon_count(lexicon, words_size_3)  \n",
    "            \n",
    "\n",
    "def process_meeting(row):\n",
    "    global topics_df\n",
    "    global running_index\n",
    "    \n",
    "    m_rows.append(row)\n",
    "    stats['total meetings'] += 1\n",
    "    if row['attended_mk_individual_ids']:\n",
    "        for mk_id in row['attended_mk_individual_ids']:\n",
    "            member_attended_meetings[mk_id] += 1\n",
    "    parts_filename = row['parts_parsed_filename']\n",
    "    if parts_filename:\n",
    "        if PROCESS_PARTS_LIMIT == -1 or (PROCESS_PARTS_LIMIT and stats['processed parts'] < PROCESS_PARTS_LIMIT):\n",
    "            initialize_meeting_data_global(row)\n",
    "            steps = (load_knesset_data('committees/meeting_protocols_parts/' + parts_filename, USE_DATA),)\n",
    "            if not USE_DATA and PROCESS_PARTS_CACHE:\n",
    "                steps = (cache(*steps, cache_path='.cache/committee-meeting-protocol-parts/' + parts_filename),)\n",
    "            steps += (process_meeting_protocol_part,)\n",
    "            Flow(*steps).process()\n",
    "            running_index += 1\n",
    "\n",
    "def add_meeting_data_to_table(row):\n",
    "    global topics_df\n",
    "    global meeting_data_global\n",
    "    #max_score = 0\n",
    "    #max_lexicon = \"None\"\n",
    "    #for lexicon_name in lexicons:\n",
    "    #    score = meeting_data_global[lexicon_name+\"_score\"]\n",
    "    #    if score > max_score:\n",
    "    #        max_score = score\n",
    "    #        max_lexicon = lexicon_name        \n",
    "    #meeting_data_global['BestTopic'] = [max_lexicon]\n",
    "    if topics_df is None:\n",
    "        topics_df = pd.DataFrame(meeting_data_global)\n",
    "    else:\n",
    "        topics_df = topics_df.append(pd.DataFrame(meeting_data_global), ignore_index=True)\n",
    "\n",
    "\n",
    "process_steps = (\n",
    "    process_meeting,\n",
    "    add_meeting_data_to_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataflows import Flow, dump_to_path\n",
    "\n",
    "Flow(*load_steps, *process_steps, dump_to_path('data/committee-meeting-attendees-parts')).process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv(\"topics_df_knesset_{}.csv\".format(KNESSET_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['BestTopic'] = df[['Diplomacy_score', 'Ecologics_score', 'Economics_score','Education_score', 'Health_score', 'Security_score']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv(\"topics_df_knesset_{}_summarized.csv\".format(KNESSET_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = df[['Year', 'BestTopic']].groupby(['Year', 'BestTopic']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"topics_output.csv\", 'w') as f:\n",
    "#    f.write(topics_df.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and print stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import deque\n",
    "#import yaml\n",
    "\n",
    "#top_attended_member_names = [member_names[mk_id] for mk_id, num_attended in\n",
    "                             deque(sorted(member_attended_meetings.items(), key=lambda kv: kv[1]), maxlen=5)]\n",
    "#print('\\n')\n",
    "#print('-- top attended members --')\n",
    "#print(top_attended_member_names)\n",
    "#print('\\n')\n",
    "#print('-- stats --')\n",
    "#print(yaml.dump(dict(stats), default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output data\n",
    "\n",
    "Output data is available in the left sidebar under data directory, you can check the datapackage.json and created csv file to explore the data and schema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
