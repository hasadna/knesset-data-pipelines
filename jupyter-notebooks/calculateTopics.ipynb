{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example flow for processing and aggregating stats about committee meeting attendees and protocol parts\n",
    "\n",
    "See the [DataFlows documentation](https://github.com/datahq/dataflows) for more details regarding the Flow object and processing functions.\n",
    "\n",
    "Feel free to modify and commit changes which demonstrate additional functionality or relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit processing of protocol parts for development\n",
    "PROCESS_PARTS_LIMIT = 20\n",
    "\n",
    "# Enable caching of protocol parts data (not efficient, should only be used for local development with sensible PROCESS_PARTS_LIMIT)\n",
    "PROCESS_PARTS_CACHE = True\n",
    "\n",
    "# Filter the meetings to be processed, these kwargs are passed along to DataFlows filter_rows processor for meetings resource\n",
    "MEETINGS_FILTER_ROWS_KWARGS = {'equals': [{'KnessetNum': 20}]}\n",
    "\n",
    "# Don'e use local data - loads everything from knesset data remote storage\n",
    "# When set to False - also enables caching, so you won't download from remote storage on 2nd run.\n",
    "USE_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_topic_to_set(topic_name):\n",
    "    lines = open(os.path.join(dir_name, topic_name + \".txt\"), 'r').readlines()\n",
    "    return set([line.strip().replace(\"\\ufeff\", \"\") for line in lines])\n",
    "\n",
    "dir_name = \"../topics/lexicons\"        \n",
    "\n",
    "files = os.listdir(dir_name)\n",
    "\n",
    "topics = [file.split('.')[0] for file in files]\n",
    "\n",
    "lexicons = {}\n",
    "for topic_name in topics:\n",
    "    lexicons[topic_name] = read_topic_to_list(topic_name)\n",
    "    \n",
    "lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/members/mk_individual/datapackage.json\n",
      "using cache data from .cache/members-mk-individual-names\n",
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/people/committees/meeting-attendees/datapackage.json\n"
     ]
    }
   ],
   "source": [
    "from dataflows import filter_rows, cache\n",
    "from datapackage_pipelines_knesset.common_flow import load_knesset_data, load_member_names\n",
    "\n",
    "# Loads a dict containing mapping between knesset member id and the member name\n",
    "member_names = load_member_names(use_data=USE_DATA)\n",
    "\n",
    "# define flow steps for loading the source committee meetings data\n",
    "# the actual loading is done later in the Flow\n",
    "load_steps = (\n",
    "    load_knesset_data('people/committees/meeting-attendees/datapackage.json', USE_DATA),\n",
    "    filter_rows(**MEETINGS_FILTER_ROWS_KWARGS)\n",
    ")\n",
    "\n",
    "if not USE_DATA:\n",
    "    # when loading from URL - enable caching which will skip loading on 2nd run\n",
    "    load_steps = (cache(*load_steps, cache_path='.cache/people-committee-meeting-attendees-knesset-20'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the datapackages which will be loaded\n",
    "\n",
    "Last command's output log should contain urls to datapackage.json files, open them and check the table schema to see the resource metadata and available fields which you can use in the processing functions.\n",
    "\n",
    "Check the [frictionlessdata docs](https://frictionlessdata.io/docs/) for more details about the datapackage file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract topics from lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataflows import Flow\n",
    "\n",
    "first = True\n",
    "running_index = 0\n",
    "\n",
    "meeting_data_global = None\n",
    "topics_df = None\n",
    "\n",
    "stats = defaultdict(int)\n",
    "member_attended_meetings = defaultdict(int)\n",
    "rows = []\n",
    "\n",
    "def initialize_meeting_data_global(meeting_row):\n",
    "    global meeting_data_global\n",
    "    #global running_index\n",
    "    meeting_data_global = pd.DataFrame()\n",
    "    #meeting_data_global['Index'] = running_index\n",
    "    meeting_data_global['KnessetNum'] = meeting_row['KnessetNum']\n",
    "    #meeting_data_global['topics'] = \";\".join(list(meeting_row['topics']))\n",
    "    #meeting_data_global['mks'] = \";\".join(list(meeting_row['mks']))\n",
    "    meeting_data_global['CommitteeSessionID'] = meeting_row['CommitteeSessionID']\n",
    "    meeting_data_global['Number'] = meeting_row['Number']\n",
    "    for topic_name in lexicons:\n",
    "        meeting_data_global[topic_name + \"_score\"] = 0\n",
    "\n",
    "    \n",
    "def word_permutations(word):\n",
    "    clean_word = word.strip()\n",
    "    permutations = [clean_word]\n",
    "    if len(word) > 1 and word.startswith('ה') or word.startswith('ב') or word.startswith('ל'):\n",
    "        permutations.append(word[1:])\n",
    "    return permutations\n",
    "\n",
    "\n",
    "def in_lexicon(word, lexicon):\n",
    "    for p in word_permutations(word):\n",
    "        if p in lexicon:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "            \n",
    "def lexicon_count(lexicon, words):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if in_lexicon(word, lexicon):\n",
    "            count += 1\n",
    "    return count\n",
    "        \n",
    "    \n",
    "def process_meeting_protocol_part(row):\n",
    "    global meeting_data_global\n",
    "    stats['processed parts'] += 1\n",
    "    words = row['body'].split() if row['body'] is not None else []\n",
    "    for topic_name, lexicon in lexicons.items():\n",
    "        meeting_data_global[topic_name + \"_score\"] += lexicon_count(lexicon, words)  \n",
    "            \n",
    "\n",
    "def process_meeting(row):\n",
    "    global topics_df\n",
    "    global meeting_data_global\n",
    "    global running_index\n",
    "    \n",
    "    stats['total meetings'] += 1\n",
    "    if row['attended_mk_individual_ids']:\n",
    "        for mk_id in row['attended_mk_individual_ids']:\n",
    "            member_attended_meetings[mk_id] += 1\n",
    "    parts_filename = row['parts_parsed_filename']\n",
    "    if parts_filename:\n",
    "        initialize_meeting_data_global(row)\n",
    "        if topics_df is None:\n",
    "            topics_df = meeting_data_global\n",
    "            print(topics_df)\n",
    "        else:\n",
    "            topics_df.append(meeting_data_global, ignore_index=True)\n",
    "            print(topics_df)\n",
    "\n",
    "        if PROCESS_PARTS_LIMIT and stats['processed parts'] < PROCESS_PARTS_LIMIT:\n",
    "            steps = (load_knesset_data('committees/meeting_protocols_parts/' + parts_filename, USE_DATA),)\n",
    "            if not USE_DATA and PROCESS_PARTS_CACHE:\n",
    "                steps = (cache(*steps, cache_path='.cache/committee-meeting-protocol-parts/' + parts_filename),)\n",
    "            steps += (process_meeting_protocol_part,)\n",
    "            Flow(*steps).process()\n",
    "\n",
    "process_steps = (process_meeting,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<datapackage.package.Package at 0x7f30418cfc88>,\n",
       " {'count_of_rows': 10256,\n",
       "  'bytes': 30129277,\n",
       "  'hash': '9bb63d3b4c724c88df1416113d0fb80c',\n",
       "  'dataset_name': None})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataflows import Flow, dump_to_path\n",
    "\n",
    "Flow(*load_steps, *process_steps, dump_to_path('data/committee-meeting-attendees-parts')).process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>KnessetNum</th>\n",
       "      <th>CommitteeSessionID</th>\n",
       "      <th>Number</th>\n",
       "      <th>Diplomacy_score</th>\n",
       "      <th>Ecologics_score</th>\n",
       "      <th>Economics_score</th>\n",
       "      <th>Education_score</th>\n",
       "      <th>Health_score</th>\n",
       "      <th>Security_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Index, KnessetNum, CommitteeSessionID, Number, Diplomacy_score, Ecologics_score, Economics_score, Education_score, Health_score, Security_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and print stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- top attended members --\n",
      "['איתן ברושי', 'מיכאל לוי', 'דוב חנין', 'משה גפני', 'אורי מקלב']\n",
      "\n",
      "\n",
      "-- stats --\n",
      "processed parts: 624\n",
      "total meetings: 9402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import yaml\n",
    "\n",
    "top_attended_member_names = [member_names[mk_id] for mk_id, num_attended in\n",
    "                             deque(sorted(member_attended_meetings.items(), key=lambda kv: kv[1]), maxlen=5)]\n",
    "print('\\n')\n",
    "print('-- top attended members --')\n",
    "print(top_attended_member_names)\n",
    "print('\\n')\n",
    "print('-- stats --')\n",
    "print(yaml.dump(dict(stats), default_flow_style=False, allow_unicode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output data\n",
    "\n",
    "Output data is available in the left sidebar under data directory, you can check the datapackage.json and created csv file to explore the data and schema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
