{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics discussed in knesset committees.\n",
    "Based on transcripts of the knesset committees.<br/>\n",
    "The work was done in the 'public knowledge workshop' hackathon and won 3rd place prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataflows import Flow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting committees texts and analyzing it.\n",
    "We used a library called fataflows since it is the one used by the 'public knowledge workshop.\n",
    "The data download is done in parts - each committee text is divided to parts and processed separately.<br/>\n",
    "Downloading each knesset data and analyzing it took arround 6 hourd per knesset.<br/>\n",
    "This is why we kept a cache of the downloaded data and saved files of the analyzed data for each knesset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit processing of protocol parts for development, -1 means no limit.\n",
    "PROCESS_PARTS_LIMIT = -1\n",
    "\n",
    "# Knesset num to query\n",
    "KNESSET_NUM = 18\n",
    "\n",
    "# Enable caching of protocol parts data (not efficient, should only be used for local development with sensible PROCESS_PARTS_LIMIT)\n",
    "PROCESS_PARTS_CACHE = True\n",
    "\n",
    "# Filter the meetings to be processed, these kwargs are passed along to DataFlows filter_rows processor for meetings resource\n",
    "MEETINGS_FILTER_ROWS_KWARGS = {'equals': [{'KnessetNum': KNESSET_NUM}]}\n",
    "\n",
    "# Don'e use local data - loads everything from knesset data remote storage\n",
    "# When set to False - also enables caching, so you won't download from remote storage on 2nd run.\n",
    "USE_DATA = False\n",
    "USE_CACHE = not USE_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file to save the extracted data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile = open(r\"Extracted_data/meetings_topics_knesset_\" + str(KNESSET_NUM) + \".csv\", 'w')\n",
    "outfile.write(\",\".join([\n",
    "    'KnessetNum',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'Day',\n",
    "    'Diplomacy_score',\n",
    "    'Ecologics_score',\n",
    "    'Economics_score',\n",
    "    'Education_score',\n",
    "    'Health_score',\n",
    "    'Security_score',\n",
    "    'CommitteeSessionID',\n",
    "    'Number',\n",
    "    'Topics',\n",
    "    'CommitteeID']) + \"\\n\")                       \n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created manually hebrew lexicons that describes the topics: <br/>\n",
    "Diplomacy, Ecologics, Economics, Education, Health, Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Diplomacy', 'Ecologics', 'Economics', 'Education', 'Health', 'Security']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_topic_to_set(topic_name):\n",
    "    lines = open(os.path.join(dir_name, topic_name + \".txt\"), 'r').readlines()\n",
    "    return set([line.strip().replace(\"\\ufeff\", \"\") for line in lines])\n",
    "\n",
    "dir_name = \"lexicons\"        \n",
    "files = os.listdir(dir_name)\n",
    "topics = [file.split('.')[0] for file in files]\n",
    "lexicons = {}\n",
    "for topic_name in topics:\n",
    "    lexicons[topic_name] = read_topic_to_set(topic_name)\n",
    "    \n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/members/mk_individual/datapackage.json\n",
      "using cache data from .cache/members-mk-individual-names\n",
      "loading from url: https://storage.googleapis.com/knesset-data-pipelines/data/people/committees/meeting-attendees/datapackage.json\n"
     ]
    }
   ],
   "source": [
    "from dataflows import filter_rows, cache\n",
    "from datapackage_pipelines_knesset.common_flow import load_knesset_data, load_member_names\n",
    "\n",
    "# Loads a dict containing mapping between knesset member id and the member name\n",
    "member_names = load_member_names(use_data=USE_DATA)\n",
    "\n",
    "# define flow steps for loading the source committee meetings data\n",
    "# the actual loading is done later in the Flow\n",
    "load_steps = (\n",
    "    load_knesset_data('people/committees/meeting-attendees/datapackage.json', USE_DATA),\n",
    "    filter_rows(**MEETINGS_FILTER_ROWS_KWARGS)\n",
    ")\n",
    "\n",
    "if USE_CACHE:\n",
    "    # when loading from URL - enable caching which will skip loading on 2nd run\n",
    "    path = '../.cache/people-committee-meeting-attendees-knesset-{}'.format(KNESSET_NUM)\n",
    "    load_steps = (cache(*load_steps, cache_path=path),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_index = 0\n",
    "meeting_data_global = None\n",
    "\n",
    "words_freq = defaultdict(int)\n",
    "stats = defaultdict(int)\n",
    "stats['processed parts'] = 0\n",
    "member_attended_meetings = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process row - extract topics apearances count from each row\n",
    "Note that each committee contain 'topics' value which is a summary of the discussed topics within a meeting. <br/>\n",
    "Each meeting topic score was calculated as the following: <br/>\n",
    "1 * topic words appearances in meeting body + \n",
    "3 * topic words appearances in meeting given 'topics' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_permutations(word):\n",
    "    clean_word = word.strip()\n",
    "    permutations = [clean_word]\n",
    "    if len(word) > 1 and word.startswith('ה') or word.startswith('ב') or word.startswith('ל'):\n",
    "        permutations.append(word[1:])\n",
    "    return permutations\n",
    "\n",
    "\n",
    "def in_lexicon(word, lexicon):\n",
    "    global words_freq\n",
    "    for p in word_permutations(word):\n",
    "        if p in lexicon:\n",
    "            words_freq[p] += 1\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "            \n",
    "def lexicon_count(lexicon, words):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if in_lexicon(word, lexicon):\n",
    "            count += 1\n",
    "    return count   \n",
    "\n",
    "\n",
    "def process_meeting_protocol_part(row):\n",
    "    global meeting_data_global\n",
    "    global running_index\n",
    "    global stats\n",
    "    stats['processed parts'] += 1\n",
    "    \n",
    "    if 'header' in row and row['header'] is not None:\n",
    "        words = row['header'].split()\n",
    "    else:\n",
    "        words = []\n",
    "    if 'body' in row and row['body'] is not None:\n",
    "        words += row['body'].split()      \n",
    "    words_size_2 = [\" \".join(words[i:i+2]) for i in range(len(words) - 2)]\n",
    "    words_size_3 = [\" \".join(words[i:i+3]) for i in range(len(words) - 3)]\n",
    "                    \n",
    "    for topic_name, lexicon in lexicons.items():\n",
    "        meeting_data_global[topic_name + \"_score\"] += lexicon_count(lexicon, words)\n",
    "        meeting_data_global[topic_name + \"_score\"] += lexicon_count(lexicon, words_size_2)  \n",
    "        meeting_data_global[topic_name + \"_score\"] += lexicon_count(lexicon, words_size_3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the meeting insights to all insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meeting_data_to_file():\n",
    "    global meeting_data_global\n",
    "    if meeting_data_global is None:\n",
    "        print(\"first run, meeting_data_global is none\")\n",
    "        return\n",
    "\n",
    "    array_to_write = [\n",
    "        meeting_data_global['KnessetNum'],\n",
    "        meeting_data_global['Year'],\n",
    "        meeting_data_global['Month'],\n",
    "        meeting_data_global['Day'],\n",
    "        meeting_data_global['Diplomacy_score'],\n",
    "        meeting_data_global['Ecologics_score'],\n",
    "        meeting_data_global['Economics_score'],\n",
    "        meeting_data_global['Education_score'],\n",
    "        meeting_data_global['Health_score'],\n",
    "        meeting_data_global['Security_score'],\n",
    "        meeting_data_global['CommitteeSessionID'],\n",
    "        meeting_data_global['Number'],\n",
    "        meeting_data_global['Topics'],\n",
    "        meeting_data_global['CommitteeID']\n",
    "    ]\n",
    "    array_to_write = [str(w).replace(\",\",\"\") for w in array_to_write]\n",
    "    outfile.write(\",\".join(array_to_write) + \"\\n\")\n",
    "            \n",
    "def add_meeting_data_to_table(row):\n",
    "    global topics_df\n",
    "    global meeting_data_global\n",
    "    if topics_df is None:\n",
    "        topics_df = pd.DataFrame(meeting_data_global)\n",
    "    else:\n",
    "        topics_df = topics_df.append(pd.DataFrame(meeting_data_global), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_score_with_meeting_given_topics(given_topics_string):\n",
    "    topic_words = given_topics_string.split()\n",
    "    topic_words_size_2 = [\" \".join(topic_words[i:i+2]) for i in range(len(topic_words) - 2)]\n",
    "    topic_words_size_3 = [\" \".join(topic_words[i:i+3]) for i in range(len(topic_words) - 3)]\n",
    "      \n",
    "    for topic_name, lexicon in lexicons.items():\n",
    "        count = lexicon_count(lexicon, topic_words) + lexicon_count(lexicon, topic_words_size_2) + lexicon_count(lexicon, topic_words_size_3)\n",
    "        meeting_data_global[topic_name + \"_score\"] = count * 3\n",
    "    \n",
    "\n",
    "def initialize_meeting_data_global(meeting_row):\n",
    "    global meeting_data_global\n",
    "    topics_exists_in_meeting_data = meeting_row['topics'] is not None\n",
    "    given_topics_string = \" ; \".join(meeting_row['topics']).replace(\",\", \"\").replace(\"\\n\", \"\") if topics_exists_in_meeting_data else \"\"\n",
    "\n",
    "    meeting_data_global = {\n",
    "        'KnessetNum': meeting_row['KnessetNum'],\n",
    "        'Year': str(meeting_row['StartDate']).split(\"-\")[0],\n",
    "        'Month': str(meeting_row['StartDate']).split(\"-\")[1],\n",
    "        'Day': str(meeting_row['StartDate']).split(\"-\")[2].split(' ')[0],\n",
    "        'CommitteeSessionID': meeting_row['CommitteeSessionID'],\n",
    "        'Number': meeting_row['Number'],\n",
    "        'Topics': given_topics_string,\n",
    "        'CommitteeID': meeting_row['CommitteeID']\n",
    "    } \n",
    "    \n",
    "    for topic_name in topics:\n",
    "        meeting_data_global[topic_name + \"_score\"] = 0\n",
    "    \n",
    "    if topics_exists_in_meeting_data:\n",
    "        update_score_with_meeting_given_topics(given_topics_string)\n",
    "\n",
    "def process_meeting(row):\n",
    "    global stats\n",
    "    global meeting_data_global\n",
    "    stats['total meetings'] += 1\n",
    "    parts_filename = row['parts_parsed_filename']\n",
    "    keep_processing = PROCESS_PARTS_LIMIT == -1 or stats['processed parts'] < PROCESS_PARTS_LIMIT\n",
    "    \n",
    "    # Process meeting\n",
    "    if parts_filename and keep_processing:\n",
    "        \n",
    "        # Add previous data to file\n",
    "        write_meeting_data_to_file()\n",
    "        \n",
    "        # Initialize new global data to file\n",
    "        initialize_meeting_data_global(row)\n",
    "        \n",
    "        # Define the meeting processing steps\n",
    "        # load data step\n",
    "        steps = (load_knesset_data('committees/meeting_protocols_parts/' + parts_filename, USE_DATA),)\n",
    "        if USE_CACHE and PROCESS_PARTS_CACHE:\n",
    "            steps = (cache(*steps, cache_path='../.cache/committee-meeting-protocol-parts/' + parts_filename),)\n",
    "            \n",
    "        # Text insights step\n",
    "        steps += (process_meeting_protocol_part,)\n",
    "        \n",
    "        # Run meeting processing steps\n",
    "        Flow(*steps).process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_steps = (\n",
    "    process_meeting,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataflows import Flow, dump_to_path\n",
    "\n",
    "Flow(*load_steps, *process_steps, dump_to_path('../data/committee-meeting-attendees-parts')).process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the lexicon words that influenced the topic scores the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words_freq.keys())\n",
    "freqs = list(words_freq.values())\n",
    "word_freq_d = {\"Lexicon words\": words,\n",
    "             \"Frquency\": freqs}\n",
    "word_freq_df = pd.DataFrame(word_freq_d)\n",
    "word_freq_df.to_csv(\"Extracted_data/words_freq_knesset_\" + str(KNESSET_NUM) + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
